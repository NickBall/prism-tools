import argparse
import csv
import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path

import requests


CACHE_DIR = "cache/mlb"

MLB_40_MAN_URL = (
    "https://statsapi.mlb.com/api/v1/teams/{team_id}/roster/{type}?hydrate=person"
)

TEAM_ID_RANGES = [range(108, 121 + 1), range(133, 147 + 1), range(158, 159)]
TEAM_IDS = set().union(*TEAM_ID_RANGES)


def fetch_40_man(team_id: int, type: str, refresh: bool):
    os.makedirs(CACHE_DIR, exist_ok=True)
    path = os.path.join(CACHE_DIR, f"{team_id}_{type}.json")

    if refresh or not os.path.exists(path):
        print(f"Downloading roster for {team_id}")
        url = MLB_40_MAN_URL.format(team_id=team_id, type=type)
        resp = requests.get(url)
        resp.raise_for_status()
        data = resp.json()
        with open(path, "w", encoding="utf-8") as f:
            json.dump(resp.json(), f, indent=2, ensure_ascii=False)
        time.sleep(1)
        return data["roster"]

    print(f"Using cached roster for {team_id}")
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)["roster"]


def normalize_name(name: str):
    parts = name.strip().split(",")
    if len(parts) == 2:
        return parts[0].strip().lower(), parts[1].strip().lower()
    return "", name.strip().lower()


def load_existing_ids(csv_path: Path):
    seen_ids = set()
    with csv_path.open() as f:
        reader = csv.DictReader(f)
        for row in reader:
            mlbam = row.get("mlbam_id", "").strip()
            if mlbam:
                seen_ids.add(mlbam)
    return seen_ids


def generate_skeleton_entries(roster_data, known_ids):
    skeletons = []
    for player in roster_data:
        person = player["person"]
        mlbam_id = str(person["id"])
        if mlbam_id in known_ids:
            continue
        birth_year = birth_month = birth_day = ""
        if "birthDate" in person:
            [birth_year, birth_month, birth_date] = (
                person["birthDate"].strip().split("-")
            )
        skeletons.append(
            {
                "prism_id": "",
                "last_name": person["lastName"].strip(),
                "first_name": person["firstName"].strip(),
                "middle_name": person.get("middleName", "").strip(),
                "name": person["lastFirstName"].strip(),
                "birth_year": birth_year,
                "birth_month": birth_month,
                "birth_day": birth_day,
                "mlbam_id": mlbam_id,
            }
        )
    return skeletons


def print_skeleton_csv(skeletons, fields):
    writer = csv.DictWriter(sys.stdout, fieldnames=fields, extrasaction="ignore")
    writer.writeheader()
    for row in skeletons:
        writer.writerow(row)


def write_issues_txt(issues: list[dict], outfile_path: str = "issues.txt") -> None:
    """
    Writes a markdown-formatted issues.txt file based on missing roster players.

    Each issue should be a dict with:
    - mlbam_id
    - name
    """
    now = datetime.utcnow().isoformat()
    header = [
        "## üîç Missing Players from MLB 40-Man Rosters",
        "",
        f"_Generated {now} UTC_",
        "",
        "The following 40-man players are missing from the PRISM Crosswalk",
        "",
        "| MLBAM ID | Name         |",
        "|----------|--------------|",
    ]

    table_rows = []
    csv_rows = []
    for issue in issues:
        table_rows.append(f"| {issue['mlbam_id']} | {issue['name']} |")
        # CSV update line (prism_id,new_fg_id)
        csv_rows.append(f"{issue['mlbam_id']}, {issue['name']}")

    csv_section = [
        "",
        "### Suggested Edits",
        "",
        "You may want to insert the following rows in `players.csv`:",
        "",
        "```csv",
        *csv_rows,
        "```",
        "",
        "Please verify manually before updating.",
        "",
        "---",
        "",
        "‚úÖ Auto-generated by `check_missing_mlb_rostered.py`",
    ]

    with open(outfile_path, "w") as f:
        f.write("\n".join(header + table_rows + csv_section))


def load_csv_header(path: Path) -> list[str]:
    with path.open() as f:
        reader = csv.reader(f)
        return next(reader)


def main(args):
    current_ids = load_existing_ids(Path(args.csv))
    team_ids = args.team_ids if args.team_ids else TEAM_IDS
    skeletons = []
    for team_id in team_ids:
        roster = fetch_40_man(team_id, args.type, args.refresh)
        skeletons.extend(generate_skeleton_entries(roster, current_ids))
    field_names = (
        args.fields.split(",") if args.fields else load_csv_header(Path(args.csv))
    )
    print("Using field ids:", field_names)
    if skeletons:
        print_skeleton_csv(skeletons, field_names)
        print(f"‚ö†Ô∏è Missing {len(skeletons)} rostered players")
        if args.issues_file:
            write_issues_txt(skeletons, args.issues_file)
        else:
            sys.exit(1)
    else:
        print("‚úÖ No missing players from rosters")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Check MLB 40-man rosters for missing PRISM players."
    )
    parser.add_argument(
        "--refresh", action="store_true", help="Force refresh and re-download data"
    )
    parser.add_argument(
        "--team-ids",
        type=int,
        metavar="N",
        nargs="+",
        required=False,
        help="MLB team ID (from MLB statsapi)",
    )
    parser.add_argument("--type", default="40Man", help="Roster type (40Man, active)")
    parser.add_argument(
        "--csv", default="data/mlb/players.csv", help="Path to current player CSV"
    )
    parser.add_argument(
        "--fields",
        help="Comma-separated list of field ids (overrides current CSV headers)",
    )
    parser.add_argument(
        "--issues-file", help="Create an issues file (for creating a GitHub Issue)"
    )
    args = parser.parse_args()

    main(args)
