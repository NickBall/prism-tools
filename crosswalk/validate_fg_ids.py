import argparse
import csv
import sys
import time
from datetime import datetime

import requests


def fg_url(id: str) -> str:
    return f"https://www.fangraphs.com/players/chase-dollander/{id}/stats"


def is_sa_id(fg_id: str) -> bool:
    return fg_id.startswith("sa")


def check_redirect(fg_id: str) -> str | None:
    url = fg_url(fg_id)
    try:
        response = requests.head(url, allow_redirects=False)
        # TODO add retries/backoff here
        response.raise_for_status()
        if 300 <= response.status_code < 400:
            location = response.headers.get("Location", "")
            if "/players/" in location and fg_id not in location:
                new_id = location.split("/")[-2]
                return new_id
    except requests.RequestException as e:
        print(f"Error checking {fg_id}: {e}", file=sys.stderr)
    return None


def write_issues_txt(issues: list[dict], outfile_path: str = "issues.txt") -> None:
    """
    Writes a markdown-formatted issues.txt file based on Fangraphs ID discrepancies.

    Each issue should be a dict with:
    - prism_id
    - name
    - old_fg_id
    - new_fg_id (optional)
    - redirected (bool)
    - note (optional)
    """
    now = datetime.utcnow().isoformat()
    header = [
        "## ðŸ§­ Fangraphs ID Redirects Detected",
        "",
        f"_Generated {now} UTC_",
        "",
        "The following player Fangraphs IDs appear to redirect.",
        "",
        "| Prism ID | Name         | Old FG ID | New FG ID |",
        "|----------|--------------|-----------|-----------|",
    ]

    table_rows = []
    csv_rows = []
    for issue in issues:
        table_rows.append(
            f"| {issue['prism_id']} | {issue['last_name']}, {issue['first_name']} "
            f"| {issue['old_fg_id']} | {issue.get('new_fg_id', '')} |"
        )
        # CSV update line (prism_id,new_fg_id)
        csv_rows.append(f"{issue['prism_id']},{issue.get('new_fg_id', '')}")

    csv_section = [
        "",
        "### Suggested Edits",
        "",
        "You may want to change the following rows in `players.csv`:",
        "",
        "```csv",
        *csv_rows,
        "```",
        "",
        "Please verify manually before updating.",
        "",
        "---",
        "",
        "âœ… Auto-generated by `validate_fg_ids.py`",
    ]

    with open(outfile_path, "w") as f:
        f.write("\n".join(header + table_rows + csv_section))


def validate_csv(
    csv_path: str, start: int = 1, quiet: bool = False, issues_file: str = None
):
    issues = []

    with open(csv_path, newline="", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for idx, row in enumerate(reader, start=1):
            if idx < start:
                continue
            fg_id = row.get("fangraphs_id", "")
            if is_sa_id(fg_id):
                if not quiet:
                    print(f"Checking {fg_id}...")
                new_id = check_redirect(fg_id)
                if new_id:
                    print(
                        f"[ERROR] Row {idx}: Fangraphs {fg_id} now redirects to ID {new_id}"
                    )
                    issues.append(
                        {
                            "prism_id": row.get("prism_id", ""),
                            "last_name": row.get("last_name", ""),
                            "first_name": row.get("first_name"),
                            "old_fg_id": fg_id,
                            "new_fg_id": new_id,
                        }
                    )
                time.sleep(1)
    if issues:
        if not quiet:
            print(f"{len(issues)} changed IDs found")
        if issues_file:
            write_issues_txt(issues, issues_file)
        sys.exit(1)
    else:
        print("No redirecting Fangraphs IDs found")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Detect outdated Fangraphs 'sa' IDs that now redirect to MLB IDs"
    )
    parser.add_argument("csv_path", help="Path to player CSV file")
    parser.add_argument(
        "--start", type=int, default=1, help="Row number to start at (1-based)"
    )
    parser.add_argument("--quiet", action="store_true", help="Essential output only")
    parser.add_argument(
        "--issues-file", help="Create an issues file (for creating a GitHub Issue)"
    )
    args = parser.parse_args()
    validate_csv(args.csv_path, args.start, args.quiet, args.issues_file)
